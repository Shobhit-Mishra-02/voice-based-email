{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0c38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import mysql.connector\n",
    "import io\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d53b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "            host='localhost',  # Replace with your MySQL host\n",
    "            user='root',  # Replace with your MySQL username\n",
    "            password='Sam@9818',  # Replace with your MySQL password\n",
    "            database='mail_db' ,\n",
    "            port= '3306'# Replace with your MySQL database name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc70746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2730468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b90f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a function to extract MFCC features from an audio file\n",
    "def extract_mfcc_features(audio_file):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    return mfccs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c3a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06bc4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_audio_data(email_id):\n",
    "    query = \"SELECT audio_1, audio_2, audio_3 FROM user_data WHERE email_id = %s\"\n",
    "    cursor.execute(query, (email_id,))\n",
    "    row = cursor.fetchone()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14eefbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_id=\"sam@gmail.com\"\n",
    "audio_data = fetch_audio_data(email_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "116f969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Replace the file paths with the actual paths to your audio recordings\n",
    "enrollment1_features = extract_mfcc_features(io.BytesIO(audio_data[0]))\n",
    "enrollment2_features = extract_mfcc_features(io.BytesIO(audio_data[1]))\n",
    "enrollment3_features = extract_mfcc_features(io.BytesIO(audio_data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c10da267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCC features for the test audio sample\n",
    "test_features = extract_mfcc_features(\"D:\\\\dataset\\\\tushar2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7cfd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the Data\n",
    "# Organize your data into training and test sets\n",
    "\n",
    "# Find the maximum number of time steps among all speakers\n",
    "max_time_steps = max(enrollment1_features.shape[1], enrollment2_features.shape[1], enrollment3_features.shape[1])\n",
    "\n",
    "# Pad or truncate the enrollment arrays to the same shape\n",
    "enrollment1_features = np.pad(enrollment1_features, ((0, 0), (0, max_time_steps - enrollment1_features.shape[1])), mode='constant')\n",
    "enrollment2_features = np.pad(enrollment2_features, ((0, 0), (0, max_time_steps - enrollment2_features.shape[1])), mode='constant')\n",
    "enrollment3_features = np.pad(enrollment3_features, ((0, 0), (0, max_time_steps - enrollment3_features.shape[1])), mode='constant')\n",
    "\n",
    "# Now, you can create X_train from these padded/truncated arrays\n",
    "X_train = np.array([enrollment1_features, enrollment2_features, enrollment3_features])\n",
    "y_train = np.array([0, 1, 2])  # Assign unique labels to each speaker (0, 1, 2 in this example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc49cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the CNN Model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddf3c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c79ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 47.1680 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 27.1020 - accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3446e-07 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.1188 - accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27e79bb2e10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d812578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Verification\n",
    "# Predict the label for the test audio sample\n",
    "predicted_label = np.argmax(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b428fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 8: Save the model in the .h5 format\n",
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f02b6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1 verified.\n"
     ]
    }
   ],
   "source": [
    "# Map the predicted label to the corresponding speaker\n",
    "speakers = {0: \"Speaker 1\", 1: \"Speaker 2\", 2: \"Speaker 3\"}\n",
    "if predicted_label in speakers:\n",
    "    print(f\"{speakers[predicted_label]} verified.\")\n",
    "else:\n",
    "    print(\"Speaker not recognized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26136355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee56f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19afae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
